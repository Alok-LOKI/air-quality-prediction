{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQdUA4dJeTuE"
      },
      "outputs": [],
      "source": [
        "# 1. Install the library\n",
        "!pip install supabase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from supabase import create_client\n",
        "\n",
        "# 2. Your credentials\n",
        "url = \"https://ahswqniqmcopyoctasrl.supabase.co\"\n",
        "key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImFoc3dxbmlxbWNvcHlvY3Rhc3JsIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjU0NjE3MDMsImV4cCI6MjA4MTAzNzcwM30.ESqXxyuPbpU-GOL6xNMB0FJJUiHv_kMe6DMeLGsZT_U\"\n",
        "\n",
        "supabase = create_client(url, key)\n",
        "\n",
        "# 3. Fetch data (make sure the table name matches what you created)\n",
        "# We fetch 4000 rows to ensure we get your full dataset\n",
        "try:\n",
        "    response = supabase.table(\"berlin\").select(\"*\").range(0, 4000).execute()\n",
        "\n",
        "    # 4. Create DataFrame\n",
        "    df = pd.DataFrame(response.data)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Connection successful, but NO DATA found. Check your RLS policies in Supabase!\")\n",
        "    else:\n",
        "        # Clean the column names (removes spaces like \" pm25\")\n",
        "        df.columns = df.columns.str.strip()\n",
        "        print(f\"Success! Imported {len(df)} rows.\")\n",
        "        print(df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# 6. SAVE THE DATASET\n",
        "df.to_csv(\"cleaned_berlin_air_quality.csv\", index=False)\n",
        "print(\"Success! Data types fixed and file saved as 'cleaned_berlin_air_quality.csv'\")\n",
        "print(df.head(30))"
      ],
      "metadata": {
        "id": "R1Y1xg3G9gpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdjMtTR3GcoK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Load your dataset\n",
        "df = pd.read_csv('/content/cleaned_berlin_air_quality.csv') # Use your actual filename\n",
        "\n",
        "# 2. Clean Column Names and Data Types\n",
        "# Remove leading/trailing spaces from column headers\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Convert pollutants to numeric (this turns empty strings/whitespace into NaN)\n",
        "cols_to_fix = ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']\n",
        "for col in cols_to_fix:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Ensure 'date' is in datetime format and sorted\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "# 3. Create a Continuous Date Range (The \"Gap Filler\")\n",
        "# This step inserts rows for missing dates like Dec 29 and 30\n",
        "full_range = pd.date_range(start=df['date'].min(), end=df['date'].max(), freq='D')\n",
        "df = df.set_index('date').reindex(full_range).reset_index().rename(columns={'index': 'date'})\n",
        "\n",
        "# 4. Interpolate Missing Values\n",
        "# This fills the NaNs for Dec 29, 30, and 31 with estimated values based on the days around them\n",
        "df[cols_to_fix] = df[cols_to_fix].interpolate(method='linear').ffill().bfill()\n",
        "\n",
        "# 5. Filter for the Last 30 Days (Inclusive)\n",
        "last_date = df['date'].max()\n",
        "start_date = last_date - pd.Timedelta(days=30)\n",
        "\n",
        "# CHANGE: Use >= so we don't skip the first day of the window\n",
        "last_30_days_df = df[(df['date'] >= start_date) & (df['date'] <= last_date)].copy()\n",
        "\n",
        "# 6. Verification & Save\n",
        "print(\"Check: Data for end of December (Fixed & Interpolated):\")\n",
        "print(last_30_days_df[last_30_days_df['date'].dt.month == 12].tail(5))\n",
        "\n",
        "last_30_days_df.to_csv('last_30_days_data.csv', index=False)\n",
        "print(\"\\nFiltered and gap-filled data saved to 'last_30_days_data.csv'\")\n",
        "\n",
        "# View the new structure\n",
        "last_30_days_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load 30 days Data\n",
        "df = pd.read_csv('/content/last_30_days_data.csv')\n",
        "\n",
        "print(\"Initial Data Head:\")\n",
        "print(df.head(31))\n",
        "print(\"\\n\\nData Info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "SR35pwdX6Mbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['so2'], inplace=True)\n",
        "df.drop(columns=['co'], inplace=True)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "8fRrvntp6YKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAXRSDseOREJ"
      },
      "outputs": [],
      "source": [
        "missing_cols = df.columns[df.isnull().any()]\n",
        "print(\"Columns with Missing Values:\")\n",
        "print(missing_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIpLMNCGO-VN"
      },
      "outputs": [],
      "source": [
        "print(df[df.isna().any(axis=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSdPG7gbPt5c"
      },
      "outputs": [],
      "source": [
        "# Replace '-' with NaN and forward fill\n",
        "df.replace('-', np.nan, inplace=True)\n",
        "df.ffill(inplace=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx-onbR9QdEM"
      },
      "outputs": [],
      "source": [
        "print(df.head(31))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2k_l5EwQ4wG"
      },
      "outputs": [],
      "source": [
        "# Select Features (Pollutant Columns)\n",
        "features = ['pm25', 'pm10', 'no2', 'o3']\n",
        "data = df[features]\n",
        "\n",
        "# Handle Missing Values (Using Simple Interpolation)\n",
        "# This fills gaps using a linear trend between known values.\n",
        "data = data.interpolate(method='linear')\n",
        "\n",
        "# Verify no more missing data\n",
        "print(\"\\nMissing values after interpolation:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\nProcessed Data Head:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZjxOEs0RGXu"
      },
      "outputs": [],
      "source": [
        "# Scaling and Sequence Generation\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Define Sequence Parameters\n",
        "N_IN = 7   # Lookback window (7 days)\n",
        "N_OUT = 2     # Prediction horizon (2 days)\n",
        "N_FEATURES = len(features) # 4 pollutants\n",
        "\n",
        "# Function to Create Sequences (Sliding Window)\n",
        "def create_sequences(data, n_in, n_out):\n",
        "    X, y = [], []\n",
        "    # Loop from the starting point up to the end minus the total length of the sequence\n",
        "    for i in range(len(data) - n_in - n_out + 1):\n",
        "        # Input sequence (7 days)\n",
        "        end_ix = i + n_in\n",
        "        X.append(data[i:end_ix, :])\n",
        "\n",
        "        # Output sequence (2 days starting right after the input ends)\n",
        "        out_ix = end_ix + n_out\n",
        "        y.append(data[end_ix:out_ix, :])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the sequences\n",
        "X, y = create_sequences(scaled_data, N_IN, N_OUT)\n",
        "\n",
        "print(f\"\\nShape of Input (X): (Samples, Lookback Days, Features) -> {X.shape}\")\n",
        "print(f\"Shape of Output (y): (Samples, Prediction Horizon, Features) -> {y.shape}\")\n",
        "\n",
        "# Chronological Train/Test Split (70% Train, 30% Test)\n",
        "# Time series data must be split chronologically.\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"Train/Test Split: {len(X_train)} training samples, {len(X_test)} test samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2Sx3PWvRe4r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Build, Train, and Evaluate the LSTM Model\n",
        "\n",
        "# Build the Model\n",
        "model = Sequential([\n",
        "    Input(shape=(N_IN, N_FEATURES)),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(N_OUT * N_FEATURES),\n",
        "    Reshape((N_OUT, N_FEATURES))\n",
        "])\n",
        "\n",
        "model.add(LSTM(\n",
        "    units=64,\n",
        "    activation='tanh',\n",
        "    return_sequences=True,\n",
        "    input_shape=(N_IN, N_FEATURES)\n",
        "))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(\n",
        "    units=32,\n",
        "    activation='tanh',\n",
        "    return_sequences=False\n",
        "))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(N_OUT * N_FEATURES))\n",
        "model.add(Reshape((N_OUT, N_FEATURES)))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "#COMPILATION\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Display Model Summary\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Training the Model\n",
        "# EarlyStopping prevents overfitting by stopping training if validation loss doesn't improve.\n",
        "callback = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=16,\n",
        "    verbose=1,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[callback]\n",
        ")\n",
        "\n",
        "# Evaluate Model on the Test Set\n",
        "test_loss, test_mae = model.evaluate(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"\\nTest Set MSE: {test_loss:.4f}\")\n",
        "print(f\"Test Set MAE: {test_mae:.4f}\")\n",
        "\n",
        "# Make Predictions on Test Set\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Inverse Transform (Rescale to original values)\n",
        "y_test_original = scaler.inverse_transform(\n",
        "    y_test.reshape(-1, N_FEATURES)\n",
        ")\n",
        "y_pred_original = scaler.inverse_transform(\n",
        "    y_pred_scaled.reshape(-1, N_FEATURES)\n",
        ")\n",
        "\n",
        "# Reshape back to 3D for comparison plots\n",
        "y_test_original = y_test_original.reshape(y_test.shape)\n",
        "y_pred_original = y_pred_original.reshape(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H30eSy67STeT"
      },
      "outputs": [],
      "source": [
        "# Final 2-Day Forecast\n",
        "\n",
        "# Ensure 'date' column is datetime and set as index for correct last_date extraction\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.set_index('date')\n",
        "\n",
        "# Prepare the Final Input\n",
        "# Take the last N_IN (7) days from the scaled dataset\n",
        "last_7_days_scaled = scaled_data[-N_IN:]\n",
        "\n",
        "# Reshape the input to match model's expected input shape: (1, N_IN, N_FEATURES)\n",
        "X_input = last_7_days_scaled.reshape(1, N_IN, N_FEATURES)\n",
        "\n",
        "# Generate the Forecast\n",
        "forecast_scaled = model.predict(X_input)\n",
        "\n",
        "# Reshape the forecast (1, N_OUT * N_FEATURES) to (N_OUT, N_FEATURES) for inverse scaling\n",
        "forecast_scaled = forecast_scaled.reshape(N_OUT, N_FEATURES)\n",
        "\n",
        "# Inverse Transform the Forecast to Original Units\n",
        "final_forecast_original = scaler.inverse_transform(forecast_scaled)\n",
        "\n",
        "# Create a Forecast DataFrame\n",
        "last_date = df.index[-1]\n",
        "forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=N_OUT, freq='D')\n",
        "\n",
        "forecast_df = pd.DataFrame(final_forecast_original,\n",
        "                           index=forecast_dates,\n",
        "                           columns=features)\n",
        "\n",
        "\n",
        "print(\"\\n\\n#####################################################\")\n",
        "print(\"## 2-DAY AIR QUALITY FORECAST (NEXT 48 HOURS) âœ… ##\")\n",
        "print(\"#####################################################\")\n",
        "print(forecast_df)\n",
        "print(\"-----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KEN0wL74RTI"
      },
      "outputs": [],
      "source": [
        "# AQI CALCULATION (POST-PROCESSING, NO MODEL CHANGE) ---\n",
        "\n",
        "aqi_breakpoints = {\n",
        "    \"pm25\": [(0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150),\n",
        "             (55.5, 150.4, 151, 200), (150.5, 250.4, 201, 300), (250.5, 500.4, 301, 500)],\n",
        "\n",
        "    \"pm10\": [(0, 54, 0, 50), (55, 154, 51, 100), (155, 254, 101, 150),\n",
        "             (255, 354, 151, 200), (355, 424, 201, 300), (425, 604, 301, 500)],\n",
        "\n",
        "    \"no2\":  [(0, 53, 0, 50), (54, 100, 51, 100), (101, 360, 101, 150),\n",
        "             (361, 649, 151, 200), (650, 1249, 201, 300), (1250, 2049, 301, 500)],\n",
        "\n",
        "    \"o3\":   [(0, 54, 0, 50), (55, 70, 51, 100), (71, 85, 101, 150),\n",
        "             (86, 105, 151, 200), (106, 200, 201, 300)]\n",
        "}\n",
        "\n",
        "def calculate_sub_aqi(pollutant, value):\n",
        "    for bp_lo, bp_hi, i_lo, i_hi in aqi_breakpoints[pollutant]:\n",
        "        if bp_lo <= value <= bp_hi:\n",
        "            return ((i_hi - i_lo)/(bp_hi - bp_lo)) * (value - bp_lo) + i_lo\n",
        "    return aqi_breakpoints[pollutant][-1][3]  # max AQI for that pollutant\n",
        "\n",
        "\n",
        "# Calculate AQI for each predicted day\n",
        "aqi_values = []\n",
        "aqi_categories = []\n",
        "\n",
        "for _, row in forecast_df.iterrows():\n",
        "    sub_indices = [\n",
        "        calculate_sub_aqi('pm25', row['pm25']),\n",
        "        calculate_sub_aqi('pm10', row['pm10']),\n",
        "        calculate_sub_aqi('no2', row['no2']),\n",
        "        calculate_sub_aqi('o3', row['o3'])\n",
        "    ]\n",
        "    final_aqi = int(max(sub_indices))\n",
        "    aqi_values.append(final_aqi)\n",
        "\n",
        "    if final_aqi <= 50:\n",
        "        aqi_categories.append(\"Good\")\n",
        "    elif final_aqi <= 100:\n",
        "        aqi_categories.append(\"Satisfactory\")\n",
        "    elif final_aqi <= 200:\n",
        "        aqi_categories.append(\"Moderate\")\n",
        "    elif final_aqi <= 300:\n",
        "        aqi_categories.append(\"Poor\")\n",
        "    elif final_aqi <= 400:\n",
        "        aqi_categories.append(\"Very Poor\")\n",
        "    else:\n",
        "        aqi_categories.append(\"Severe\")\n",
        "\n",
        "# Add AQI to forecast table\n",
        "forecast_df[\"AQI\"] = aqi_values\n",
        "forecast_df[\"AQI_Category\"] = aqi_categories\n",
        "\n",
        "print(\"## FINAL AQI FORECAST (NEXT 2 DAYS) ##\")\n",
        "print(\"#############################################\")\n",
        "print(forecast_df)\n",
        "print(\"---------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DAY-WISE AQI FOR HISTORICAL + PREDICTED DAYS\n",
        "\n",
        "# calculate AQI for a dataframe\n",
        "def calculate_daily_aqi(dataframe):\n",
        "    aqi_list = []\n",
        "    category_list = []\n",
        "\n",
        "    for _, row in dataframe.iterrows():\n",
        "        sub_indices = [\n",
        "            calculate_sub_aqi('pm25', row['pm25']),\n",
        "            calculate_sub_aqi('pm10', row['pm10']),\n",
        "            calculate_sub_aqi('no2', row['no2']),\n",
        "            calculate_sub_aqi('o3', row['o3'])\n",
        "        ]\n",
        "\n",
        "        final_aqi = int(max(sub_indices))\n",
        "        aqi_list.append(final_aqi)\n",
        "\n",
        "        if final_aqi <= 50:\n",
        "            category_list.append(\"Good\")\n",
        "        elif final_aqi <= 100:\n",
        "            category_list.append(\"Satisfactory\")\n",
        "        elif final_aqi <= 200:\n",
        "            category_list.append(\"Moderate\")\n",
        "        elif final_aqi <= 300:\n",
        "            category_list.append(\"Poor\")\n",
        "        elif final_aqi <= 400:\n",
        "            category_list.append(\"Very Poor\")\n",
        "        else:\n",
        "            category_list.append(\"Severe\")\n",
        "\n",
        "    return aqi_list, category_list\n",
        "\n",
        "\n",
        "# Calculate AQI for historical data\n",
        "historical_df = df[features].copy()\n",
        "historical_df[\"AQI\"], historical_df[\"AQI_Category\"] = calculate_daily_aqi(historical_df)\n",
        "\n",
        "# Calculate AQI for predicted data\n",
        "predicted_df = forecast_df.copy()\n",
        "predicted_df[\"AQI\"], predicted_df[\"AQI_Category\"] = calculate_daily_aqi(predicted_df)\n",
        "\n",
        "# Combine historical + predicted AQI\n",
        "final_aqi_df = pd.concat([historical_df, predicted_df])\n",
        "\n",
        "print(\"##  DAY-WISE AQI (HISTORICAL + PREDICTED DAYS)  ##\")\n",
        "print(\"########################################################\")\n",
        "print(final_aqi_df)\n",
        "print(\"--------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "msz_fHqDtayq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efd7faa7"
      },
      "source": [
        "final_aqi_df.reset_index().rename(columns={'index': 'date'}).to_csv('historical_and_predicted_aqi.csv', index=False)\n",
        "print(\"Final AQI historical and predicted data saved to 'historical_and_predicted_aqi.csv'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r3MN6JYeTuQ"
      },
      "outputs": [],
      "source": [
        "# 1. Prepare the dataframe for Supabase\n",
        "# Reset index so 'date' becomes a column, and rename it correctly\n",
        "upload_df = final_aqi_df.reset_index().rename(columns={'index': 'date'})\n",
        "upload_df.columns = upload_df.columns.str.strip().str.lower() # Standardize names\n",
        "\n",
        "# 2. Convert to list of dictionaries (Supabase format)\n",
        "# We ensure the 'date' column is string format YYYY-MM-DD\n",
        "upload_df['date'] = upload_df['date'].astype(str)\n",
        "\n",
        "data_to_insert = upload_df.to_dict(orient='records')\n",
        "\n",
        "# 3. Bulk Insert into the new table\n",
        "try:\n",
        "    # We use .insert() with the list of dictionaries\n",
        "    response = supabase.table(\"berlin_aqi_results\").insert(data_to_insert).execute()\n",
        "    print(f\"Successfully inserted {len(data_to_insert)} rows into 'berlin_aqi_results'!\")\n",
        "except Exception as e:\n",
        "    print(f\"Insertion failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# 1. INDIVIDUAL POLLUTANT PLOTS\n",
        "pollutants = ['pm25', 'pm10', 'no2', 'o3']\n",
        "\n",
        "for pollutant in pollutants:\n",
        "    fig = px.line(df,\n",
        "                  x=df.index,\n",
        "                  y=pollutant,\n",
        "                  title=f'{pollutant.upper()} Over Time (Last Month)',\n",
        "                  markers=True,\n",
        "                  template='plotly_dark')\n",
        "\n",
        "    # Customizing layout to match a deep black aesthetic\n",
        "    fig.update_traces(line=dict(color='white', width=2), marker=dict(size=6))\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title=pollutant.upper(),\n",
        "        plot_bgcolor='black',\n",
        "        paper_bgcolor='black',\n",
        "        xaxis=dict(showgrid=True, gridcolor='#333333', gridwidth=0.5),\n",
        "        yaxis=dict(showgrid=True, gridcolor='#333333', gridwidth=0.5),\n",
        "        height=400,\n",
        "        margin=dict(l=40, r=40, t=60, b=40)\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. AQI TREND (HISTORICAL + PREDICTED)\n",
        "fig_aqi = go.Figure()\n",
        "\n",
        "# Main AQI line\n",
        "fig_aqi.add_trace(go.Scatter(\n",
        "    x=final_aqi_df.index,\n",
        "    y=final_aqi_df[\"AQI\"],\n",
        "    mode='lines+markers',\n",
        "    name='AQI',\n",
        "    line=dict(color='cyan', width=2),\n",
        "    marker=dict(size=6)\n",
        "))\n",
        "\n",
        "# FIX: Convert Timestamp to milliseconds to prevent the TypeError in add_vline\n",
        "prediction_start_date = forecast_df.index[0].timestamp() * 1000\n",
        "\n",
        "# Vertical line showing prediction start\n",
        "fig_aqi.add_vline(\n",
        "    x=prediction_start_date,\n",
        "    line_dash=\"dash\",\n",
        "    line_color=\"red\",\n",
        "    annotation_text=\"Prediction Start\",\n",
        "    annotation_font_color=\"red\",\n",
        "    annotation_position=\"top left\"\n",
        ")\n",
        "\n",
        "# Optional: Shaded region for Predicted Forecast\n",
        "fig_aqi.add_vrect(\n",
        "    x0=prediction_start_date,\n",
        "    x1=final_aqi_df.index[-1].timestamp() * 1000,\n",
        "    fillcolor=\"rgba(255, 0, 0, 0.1)\",\n",
        "    layer=\"below\",\n",
        "    line_width=0\n",
        ")\n",
        "\n",
        "# Dark Layout Customization\n",
        "fig_aqi.update_layout(\n",
        "    title='AQI Trend (Historical + Predicted)',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='AQI Value',\n",
        "    template='plotly_dark',\n",
        "    plot_bgcolor='black',\n",
        "    paper_bgcolor='black',\n",
        "    xaxis=dict(showgrid=True, gridcolor='#333333', gridwidth=0.5),\n",
        "    yaxis=dict(showgrid=True, gridcolor='#333333', gridwidth=0.5),\n",
        "    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
        "    height=500,\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig_aqi.show()"
      ],
      "metadata": {
        "id": "Ja-4JjrdJ226"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}